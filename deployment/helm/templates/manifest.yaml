apiVersion: v1
kind: ConfigMap
metadata:
  name: foldy-configmap
data:
  ENV: "production"
  DEBUG: "False"
  OAUTH_REDIRECT_URI: "https://{{ required "Must specify FoldyDomain" .Values.FoldyDomain }}/api/authorize"
  BACKEND_URL: "https://{{ required "Must specify FoldyDomain" .Values.FoldyDomain }}"
  FRONTEND_URL: "https://{{ required "Must specify FoldyDomain" .Values.FoldyDomain }}"
  FOLDY_USER_EMAIL_DOMAIN: "{{ .Values.FoldyUserEmailDomain }}"
  FOLDY_ADMIN_UPGRADE_LIST: "{{ .Values.FoldyAdminUpgradeList }}"
  FOLDY_STORAGE_TYPE: "Cloud"
  FOLDY_GCLOUD_PROJECT: "{{ required "Must specify GoogleProjectId" .Values.GoogleProjectId }}"
  FOLDY_GSTORAGE_DIR: "{{ required "Must specify FoldyGstorageDir" .Values.FoldyGstorageDir }}"
  RQ_REDIS_URL: "redis://redis:6379/0"
  PORT: "8080"  # Specifies the backend port.

---
apiVersion: v1
kind: Secret
metadata:
  name: foldy-secret
type: Opaque
stringData:
  SECRET_KEY: "{{ required "Must specify SECRET_KEY" .Values.Secrets.SECRET_KEY }}"
  GOOGLE_CLIENT_ID: "{{ required "Must specify GOOGLE_CLIENT_ID" .Values.Secrets.GOOGLE_CLIENT_ID }}"
  GOOGLE_CLIENT_SECRET: "{{ required "Must specify GOOGLE_CLIENT_SECRET" .Values.Secrets.GOOGLE_CLIENT_SECRET }}"
  EMAIL_USERNAME: "{{ required "Must specify EMAIL_USERNAME" .Values.Secrets.EMAIL_USERNAME }}"
  EMAIL_PASSWORD: "{{ required "Must specify EMAIL_PASSWORD" .Values.Secrets.EMAIL_PASSWORD }}"
  DATABASE_URL: "{{ required "Must specify DATABASE_URL" .Values.Secrets.DATABASE_URL }}"

---
# A service account. This is needed in gcloud world to associate with a
# gcloud service account. This mechanism is how we access postgres.
apiVersion: v1
kind: ServiceAccount
metadata:
  annotations:
    iam.gke.io/gcp-service-account: "{{ required "Must specify ServiceAccount" .Values.ServiceAccount }}@{{ .Values.GoogleProjectId }}.iam.gserviceaccount.com"
  name: foldy-ksa
  namespace: default

---
apiVersion: v1
kind: Service
metadata:
  name: redis
  labels:
    app: redis
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis

---
apiVersion: apps/v1 #  for k8s versions before 1.9.0 use apps/v1beta2  and before 1.8.0 use extensions/v1beta1
kind: Deployment
metadata:
  name: redis
spec:
  selector:
    matchLabels:
      app: redis
  replicas: 1
  template:
    metadata:
      labels:
        app: redis
    spec:
      containers:
      - name: master
        image: redis:6.2.7-alpine # k8s.gcr.io/redis:e2e  # or just image: redis
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379


---
# GKE Service configuration
# See https://cloud.google.com/kubernetes-engine/docs/how-to/ingress-features
apiVersion: cloud.google.com/v1
kind: BackendConfig
metadata:
  name: backend-backendconfig
spec:
  timeoutSec: 90
  # Enable logging for now, to debug.
  logging:
    enable: true
  healthCheck:
    requestPath: /healthz
    port: 8080
  securityPolicy:
    name: "{{ .Values.SecurityPolicy }}"
---
apiVersion: v1
kind: Service
metadata:
  name: backend
  labels:
    app: backend
  annotations:
    cloud.google.com/backend-config: '{"default": "backend-backendconfig"}'
spec:
  ports:
  - port: 8080
    targetPort: 8080
  selector:
    app: backend
---
apiVersion: apps/v1 #  for k8s versions before 1.9.0 use apps/v1beta2  and before 1.8.0 use extensions/v1beta1
kind: Deployment
metadata:
  name: backend
  labels:
    app: backend
spec:
  selector:
    matchLabels:
      app: backend
  replicas: 2
  strategy:
    rollingUpdate:
      maxSurge: 75%
      maxUnavailable: 75%
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: backend
    spec:
      serviceAccountName: foldy-ksa

      nodeSelector:
        iam.gke.io/gke-metadata-server-enabled: "true"
        # cloud.google.com/gke-nodepool: generalnodes

      containers:
      - name: master
        image: {{ .Values.GoogleCloudRegion }}-docker.pkg.dev/{{ .Values.GoogleProjectId }}/{{ .Values.ArtifactRepo }}/backend:{{  .Values.ImageVersion }}
        env:
        - name: PORT
          value: "8080"
        envFrom:
        - configMapRef:
            name: foldy-configmap
        - secretRef:
            name: foldy-secret
        resources:
          requests:
            cpu: 1000m
            memory: 1Gi
          limits:
            cpu: 1000m
            memory: 1Gi
        ports:
        - name: backend-port
          containerPort: 8080

---
# Monitoring!
apiVersion: monitoring.googleapis.com/v1
kind: PodMonitoring
metadata:
  name: prom-backend
spec:
  selector:
    matchLabels:
      app: backend
  endpoints:
  - port: backend-port
    interval: 30s
    path: /metrics

---
# GKE Service configuration
# See https://cloud.google.com/kubernetes-engine/docs/how-to/ingress-features
apiVersion: cloud.google.com/v1
kind: BackendConfig
metadata:
  name: frontend-backendconfig
spec:
  securityPolicy:
    name: "{{ .Values.SecurityPolicy }}"
---
apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: frontend
  annotations:
    cloud.google.com/backend-config: '{"default": "frontend-backendconfig"}'
spec:
  ports:
  - port: 80
    targetPort: 80
  selector:
    app: frontend
  type: NodePort
---
apiVersion: apps/v1 #  for k8s versions before 1.9.0 use apps/v1beta2  and before 1.8.0 use extensions/v1beta1
kind: Deployment
metadata:
  name: frontend
spec:
  selector:
    matchLabels:
      app: frontend
  replicas: 1
  template:
    metadata:
      labels:
        app: frontend
    spec:
      containers:
      - name: master
        image: {{ .Values.GoogleCloudRegion }}-docker.pkg.dev/{{ .Values.GoogleProjectId }}/{{ .Values.ArtifactRepo }}/frontend:{{  .Values.ImageVersion }}
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: frontend-ingress
  annotations:
    # If the class annotation is not specified it defaults to "gce".
    kubernetes.io/ingress.global-static-ip-name: {{ .Values.StaticIpName }}
    networking.gke.io/managed-certificates: foldy-local-cert
    kubernetes.io/ingress.class: gce
    kubernetes.io/ingress.allow-http: "false"
spec:
  rules:
  - http:
      paths:
      - path: /*
        pathType: ImplementationSpecific
        backend:
          service:
            name: frontend
            port:
              number: 80
      - path: /api/*
        pathType: ImplementationSpecific
        backend:
          service:
            name: backend
            port:
              number: 8080
      - path: /rq/*
        pathType: ImplementationSpecific
        backend:
          service:
            name: backend
            port:
              number: 8080
      - path: /admin/*
        pathType: ImplementationSpecific
        backend:
          service:
            name: backend
            port:
              number: 8080

---
apiVersion: networking.gke.io/v1
kind: ManagedCertificate
metadata:
  name: foldy-local-cert
spec:
  domains:
    - {{ required "Must specify FoldyCertificateDomain." .Values.FoldyCertificateDomain }}


---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  namespace: default
  name: foldydbs
spec:
  storageClassName: "standard-rwo"
  accessModes:
    - ReadOnlyMany
  dataSource:
    name: foldydbs-rw
    kind: PersistentVolumeClaim
  resources:
    requests:
      storage: 3000Gi


---
{{ include "foldy.createWorkerPool" (deepCopy . | merge (dict "RqQueueName" "cpu") | merge (dict "ImageName" "worker")) }}

---
{{ include "foldy.createWorkerPool" (deepCopy . | merge (dict "RqQueueName" "esm") | merge (dict "ImageName" "worker_esm")) }}

---
{{ include "foldy.createWorkerPool" (deepCopy . | merge (dict "RqQueueName" "emailparrot") | merge (dict "ImageName" "worker")) }}

---
{{ include "foldy.createWorkerPool" (deepCopy . | merge (dict "RqQueueName" "gpu") | merge (dict "ImageName" "worker")) }}

---
{{ include "foldy.createWorkerPool" (deepCopy . | merge (dict "RqQueueName" "biggpu") | merge (dict "ImageName" "worker")) }}

---
# Prometheus frontend proxy, based on instructions here:
# https://cloud.google.com/stackdriver/docs/managed-prometheus/query#promui-deploy
apiVersion: apps/v1
kind: Deployment
metadata:
  name: prom-frontend
spec:
  replicas: 1
  selector:
    matchLabels:
      app: prom-frontend
  template:
    metadata:
      labels:
        app: prom-frontend
    spec:
      serviceAccountName: foldy-ksa

      automountServiceAccountToken: true
      nodeSelector:
        kubernetes.io/os: linux
        kubernetes.io/arch: amd64
      containers:
      - name: prom-frontend
        image: "gke.gcr.io/prometheus-engine/frontend:v0.4.3-gke.0"
        args:
        - "--web.listen-address=:9090"
        - "--query.project-id={{ .Values.GoogleProjectId }}"
        ports:
        - name: web
          containerPort: 9090
        readinessProbe:
          httpGet:
            path: /-/ready
            port: web
        livenessProbe:
          httpGet:
            path: /-/healthy
            port: web
---
apiVersion: v1
kind: Service
metadata:
  name: prom-frontend
spec:
  clusterIP: None
  selector:
    app: prom-frontend
  ports:
  - name: web
    port: 9090

---
# The following daemonset installs nvidia drivers on the nodes in the GPU
# worker nodepools when the first spin up. 
# 
# https://cloud.google.com/kubernetes-engine/docs/how-to/gpus
#
# curl https://raw.githubusercontent.com/GoogleCloudPlatform/container-engine-accelerators/master/nvidia-driver-installer/cos/daemonset-preloaded-latest.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: nvidia-driver-installer
  namespace: kube-system
  labels:
    k8s-app: nvidia-driver-installer
spec:
  selector:
    matchLabels:
      k8s-app: nvidia-driver-installer
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        name: nvidia-driver-installer
        k8s-app: nvidia-driver-installer
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: cloud.google.com/gke-accelerator
                operator: Exists
      tolerations:
      - operator: "Exists"
      hostNetwork: true
      hostPID: true
      volumes:
      - name: dev
        hostPath:
          path: /dev
      - name: vulkan-icd-mount
        hostPath:
          path: /home/kubernetes/bin/nvidia/vulkan/icd.d
      - name: nvidia-install-dir-host
        hostPath:
          path: /home/kubernetes/bin/nvidia
      - name: root-mount
        hostPath:
          path: /
      - name: cos-tools
        hostPath:
          path: /var/lib/cos-tools
      initContainers:
      - image: "cos-nvidia-installer:fixed"
        imagePullPolicy: Never
        name: nvidia-driver-installer
        resources:
          requests:
            cpu: "0.15"
        securityContext:
          privileged: true
        env:
        - name: NVIDIA_INSTALL_DIR_HOST
          value: /home/kubernetes/bin/nvidia
        - name: NVIDIA_INSTALL_DIR_CONTAINER
          value: /usr/local/nvidia
        - name: VULKAN_ICD_DIR_HOST
          value: /home/kubernetes/bin/nvidia/vulkan/icd.d
        - name: VULKAN_ICD_DIR_CONTAINER
          value: /etc/vulkan/icd.d
        - name: ROOT_MOUNT_DIR
          value: /root
        - name: COS_TOOLS_DIR_HOST
          value: /var/lib/cos-tools
        - name: COS_TOOLS_DIR_CONTAINER
          value: /build/cos-tools
        volumeMounts:
        - name: nvidia-install-dir-host
          mountPath: /usr/local/nvidia
        - name: vulkan-icd-mount
          mountPath: /etc/vulkan/icd.d
        - name: dev
          mountPath: /dev
        - name: root-mount
          mountPath: /root
        - name: cos-tools
          mountPath: /build/cos-tools
        command: ['/cos-gpu-installer', 'install', '--version=latest']
      containers:
      - image: "gcr.io/google-containers/pause:2.0"
        name: pause

---
# Allow metrics-server to be rescheduled, or else nodes don't always scale down.
#
# https://github.com/kubernetes/autoscaler/issues/2377
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: metrics-server-pdb
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      k8s-app: metrics-server